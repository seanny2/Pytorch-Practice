{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이진분류\n",
    "\n",
    "### 로지스틱 회귀\n",
    "- 선형 모델 + Sigmoid 활성함수\n",
    "- 이진교차엔트로피 손실함수 (BCE)\n",
    "\n",
    "### 평가 지표\n",
    "- 정확도: 전체 예측 개수와 실제로 맞춘 개수의 비율\n",
    "- 정밀도: 양성 클래스로 예측된 것들 중에서 실제 양성 클래스의 개수\n",
    "- 재현율: 실제 양성 클래스 중에서 양성으로 예측한 개수\n",
    "- 정밀도와 재현율은 서로 반비례하다.\n",
    "\n",
    "### 종합 지표\n",
    "- F1 점수: 정밀도와 재현율의 조화 평\n",
    "- AUROC: 변화하는 임계값에 따른 모델의 성능을 측정하기 위해 개발된 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이진분류 심층신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# 유방암 데이터셋 불러오기\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['class'] = cancer.target\n",
    "\n",
    "# 입출력 데이터 구성\n",
    "data = torch.from_numpy(df.values).float()\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1:]\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# Train / Valid / Test ratio\n",
    "ratios = [.6, .2, .2]\n",
    "\n",
    "train_cnt = int(data.size(0) * ratios[0])\n",
    "valid_cnt = int(data.size(0) * ratios[1])\n",
    "test_cnt = data.size(0) - train_cnt - valid_cnt\n",
    "cnts = [train_cnt, valid_cnt, test_cnt]\n",
    "\n",
    "print(\"Train %d / Valid %d / Test %d samples.\" % (train_cnt, valid_cnt, test_cnt))\n",
    "\n",
    "# 데이터셋 샘플 셔플\n",
    "indices = torch.randperm(data.size(0))\n",
    "\n",
    "x = torch.index_select(x, dim=0, index=indices)\n",
    "y = torch.index_select(y, dim=0, index=indices)\n",
    "\n",
    "x = x.split(cnts, dim=0)\n",
    "y = y.split(cnts, dim=0)\n",
    "\n",
    "for x_i, y_i in zip(x, y):\n",
    "    print(x_i.size(), y_i.size())\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x[0].numpy())\n",
    "\n",
    "x = [torch.from_numpy(scaler.transform(x[0].numpy())).float(),\n",
    "     torch.from_numpy(scaler.transform(x[1].numpy())).float(),\n",
    "     torch.from_numpy(scaler.transform(x[2].numpy())).float()]\n",
    "\n",
    "# 모델 구성\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x[0].size(-1), 5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5, 4),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(4, 3),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(3, 2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(2, y[0].size(-1)),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "n_epochs = 10000\n",
    "batch_size = 32\n",
    "print_interval = 10\n",
    "early_stop = 100\n",
    "\n",
    "lowest_loss = np.inf\n",
    "best_model = None\n",
    "lowest_epoch = np.inf\n",
    "\n",
    "train_history, valid_history = [], []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    indices = torch.randperm(x[0].size(0))\n",
    "    x_ = torch.index_select(x[0], dim=0, index=indices)\n",
    "    y_ = torch.index_select(y[0], dim=0, index=indices)\n",
    "    \n",
    "    x_ = x_.split(batch_size, dim=0)\n",
    "    y_ = y_.split(batch_size, dim=0)\n",
    "    \n",
    "    train_loss, valid_loss = 0, 0\n",
    "    y_hat = []\n",
    "    \n",
    "    for x_i, y_i in zip(x_, y_):\n",
    "        y_hat_i = model(x_i)\n",
    "        loss = F.binary_cross_entropy(y_hat_i, y_i) # BCE 손실함수\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()        \n",
    "        train_loss += float(loss)\n",
    "\n",
    "    train_loss = train_loss / len(x_)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        x_ = x[1].split(batch_size, dim=0)\n",
    "        y_ = y[1].split(batch_size, dim=0)\n",
    "        \n",
    "        valid_loss = 0\n",
    "        \n",
    "        for x_i, y_i in zip(x_, y_):\n",
    "            y_hat_i = model(x_i)\n",
    "            loss = F.binary_cross_entropy(y_hat_i, y_i)\n",
    "            \n",
    "            valid_loss += float(loss)\n",
    "            \n",
    "            y_hat += [y_hat_i]\n",
    "            \n",
    "    # 검증 데이터셋 손실 값\n",
    "    valid_loss = valid_loss / len(x_)\n",
    "    \n",
    "    train_history += [train_loss]\n",
    "    valid_history += [valid_loss]\n",
    "        \n",
    "    if (i + 1) % print_interval == 0:\n",
    "        print('Epoch %d: train loss=%.4e  valid_loss=%.4e  lowest_loss=%.4e' % (\n",
    "            i + 1,\n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            lowest_loss,\n",
    "        ))\n",
    "        \n",
    "    if valid_loss <= lowest_loss:\n",
    "        lowest_loss = valid_loss\n",
    "        lowest_epoch = i\n",
    "        \n",
    "        best_model = deepcopy(model.state_dict())\n",
    "    else:\n",
    "        if early_stop > 0 and lowest_epoch + early_stop < i + 1:\n",
    "            print(\"There is no improvement during last %d epochs.\" % early_stop)\n",
    "            break\n",
    "\n",
    "print(\"The best validation loss from epoch %d: %.4e\" % (lowest_epoch + 1, lowest_loss))\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 (손실)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 곡선 확인\n",
    "plot_from = 2\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.grid(True)\n",
    "plt.title(\"Train / Valid Loss History\")\n",
    "plt.plot(\n",
    "    range(plot_from, len(train_history)), train_history[plot_from:],\n",
    "    range(plot_from, len(valid_history)), valid_history[plot_from:],\n",
    ")\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# 테스트 데이터셋 평균 손실 값\n",
    "test_loss = 0\n",
    "y_hat = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_ = x[2].split(batch_size, dim=0)\n",
    "    y_ = y[2].split(batch_size, dim=0)\n",
    "\n",
    "    for x_i, y_i in zip(x_, y_):\n",
    "        y_hat_i = model(x_i)\n",
    "        loss = F.binary_cross_entropy(y_hat_i, y_i)\n",
    "\n",
    "        test_loss += loss # Gradient is already detached.\n",
    "\n",
    "        y_hat += [y_hat_i]\n",
    "\n",
    "test_loss = test_loss / len(x_)\n",
    "y_hat = torch.cat(y_hat, dim=0)\n",
    "\n",
    "sorted_history = sorted(zip(train_history, valid_history),\n",
    "                        key=lambda x: x[1])\n",
    "\n",
    "print(\"Train loss: %.4e\" % sorted_history[0][0])\n",
    "print(\"Valid loss: %.4e\" % sorted_history[0][1])\n",
    "print(\"Test loss: %.4e\" % test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 (정확도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 정확도 계산: 분류 실습에서만 가능\n",
    "# 출력값이 0.5보다 크면 1로 분류한 것으로 간주\n",
    "correct_cnt = (y[2] == (y_hat > .5)).sum()\n",
    "total_cnt = float(y[2].size(0))\n",
    "\n",
    "print('Test Accuracy: %.4f' % (correct_cnt / total_cnt))\n",
    "\n",
    "# 클래스별 히스토그램\n",
    "# 참값 예측값 분포 확인\n",
    "df = pd.DataFrame(torch.cat([y[2], y_hat], dim=1).detach().numpy(),\n",
    "                  columns=[\"y\", \"y_hat\"])\n",
    "\n",
    "sns.histplot(df, x='y_hat', hue='y', bins=50, stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 (AUROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUROC: 두 클래스 분포가 확연히 나뉠 수록 높은 값을 얻음.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "print(roc_auc_score(df.values[:, 0], df.values[:, 1]))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(df.values[:, 0], df.values[:, 1])\n",
    "plt.plot([0,1], [0,1], \"k--\", \"r+\")\n",
    "plt.plot(fpr, tpr, label=\"ROC curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4be86c8dc50e04ba973d8ce46680464486089f3e39fa7122143c61b5c28756ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
