{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증하기\n",
    "\n",
    "### 오버피팅\n",
    "- 일반화를 방해 -> 예측 융통성 부족\n",
    "- 검증 데이터셋 도입하여 해결\n",
    "    - 검증은 최적화를 진행하지 않음\n",
    "- 테스트 데이터셋을 추가하여 검증 데이터로의 편향을 막음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# 캘리포니아 주택 정보\n",
    "california = fetch_california_housing()\n",
    "df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "df[\"Target\"] = california.target\n",
    "\n",
    "# 입출력 데이터 구성\n",
    "data = torch.from_numpy(df.values).float()\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1:]\n",
    "print(x.size(), y.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검증 데이터셋\n",
    "\n",
    "##### 학습 : 검증 : 평가 = 6 : 2 : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습, 검증, 평가 데이터 비율 선정\n",
    "ratios = [.6, .2, .2]\n",
    "\n",
    "# 각 데이터셋에 속하는 데이터 개수 계산\n",
    "train_cnt = int(data.size(0) * ratios[0])\n",
    "valid_cnt = int(data.size(0) * ratios[1])\n",
    "test_cnt = data.size(0) - train_cnt - valid_cnt\n",
    "cnts = [train_cnt, valid_cnt, test_cnt]\n",
    "\n",
    "print(\"Train %d / Valid %d / Test %d samples.\" % (train_cnt, valid_cnt, test_cnt))\n",
    "\n",
    "# 입출력 데이터 임의의 순서로 섞기\n",
    "indices = torch.randperm(data.size(0))\n",
    "x = torch.index_select(x, dim=0, index=indices)\n",
    "y = torch.index_select(y, dim=0, index=indices)\n",
    "\n",
    "# 학습, 검증, 평가 데이터로 나누기\n",
    "x = list(x.split(cnts, dim=0))\n",
    "y = y.split(cnts, dim=0)\n",
    "\n",
    "for x_i, y_i in zip(x, y):\n",
    "    print(x_i.size(), y_i.size())\n",
    "\n",
    "# 데이터셋 정규화 하기: 학습 데이터에 대해 피팅된 스케일러(scaler)를 이용하여 남저ㅣ 데이터도 정규화\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x[0].numpy()) # You must fit with train data only.\n",
    "\n",
    "x[0] = torch.from_numpy(scaler.transform(x[0].numpy())).float()\n",
    "x[1] = torch.from_numpy(scaler.transform(x[1].numpy())).float()\n",
    "x[2] = torch.from_numpy(scaler.transform(x[2].numpy())).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 및 옵티파이저 설정\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x[0].size(-1), 8),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(8, 7),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(7, 6),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(6, 5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5, 3),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(3, 3),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(3, y[0].size(-1)),\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 세팅값 설정\n",
    "n_epochs = 4000\n",
    "batch_size = 256\n",
    "print_interval = 100\n",
    "\n",
    "# 최저 검증 손실 모델 복원을 위한 세팅값 설정\n",
    "from copy import deepcopy\n",
    "\n",
    "lowest_loss = np.inf\n",
    "best_model = None   # 최상의 모델\n",
    "\n",
    "early_stop = 100    # 학습의 의미가 없으면 일찍 종료\n",
    "lowest_epoch = np.inf\n",
    "\n",
    "# 학습 시작\n",
    "# 훈련, 검증 데이터 손실값 리스트 -> 손실 그래프\n",
    "train_history, valid_history = [], []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # 임의의 순서로 미니배치로 나누기\n",
    "    indices = torch.randperm(x[0].size(0))\n",
    "    x_ = torch.index_select(x[0], dim=0, index=indices)\n",
    "    y_ = torch.index_select(y[0], dim=0, index=indices)\n",
    "    \n",
    "    x_ = x_.split(batch_size, dim=0)\n",
    "    y_ = y_.split(batch_size, dim=0)\n",
    "    \n",
    "    train_loss, valid_loss = 0, 0\n",
    "    y_hat = []\n",
    "    \n",
    "    # 경사하강법\n",
    "    for x_i, y_i in zip(x_, y_):\n",
    "        y_hat_i = model(x_i)\n",
    "        loss = F.mse_loss(y_hat_i, y_i)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "        train_loss += float(loss)\n",
    "\n",
    "    # 손실값\n",
    "    train_loss = train_loss / len(x_)\n",
    "\n",
    "    # 검증시작: 과적합 방지, 미분 수행 x\n",
    "    with torch.no_grad():\n",
    "        x_ = x[1].split(batch_size, dim=0)\n",
    "        y_ = y[1].split(batch_size, dim=0)\n",
    "        \n",
    "        valid_loss = 0\n",
    "        \n",
    "        for x_i, y_i in zip(x_, y_):\n",
    "            y_hat_i = model(x_i)\n",
    "            loss = F.mse_loss(y_hat_i, y_i)\n",
    "            \n",
    "            valid_loss += loss\n",
    "            \n",
    "            y_hat += [y_hat_i]\n",
    "            \n",
    "    valid_loss = valid_loss / len(x_)\n",
    "    \n",
    "    train_history += [train_loss]\n",
    "    valid_history += [valid_loss]\n",
    "        \n",
    "    if (i + 1) % print_interval == 0:\n",
    "        print('Epoch %d: train loss=%.4e  valid_loss=%.4e  lowest_loss=%.4e' % (\n",
    "            i + 1,\n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            lowest_loss,\n",
    "        ))\n",
    "\n",
    "    # 최소 검증 손실값 갱신 및 모델 저장: 현재 검증 손실값과 비교\n",
    "    if valid_loss <= lowest_loss:\n",
    "        lowest_loss = valid_loss\n",
    "        lowest_epoch = i\n",
    "        \n",
    "        # 최소 검증 손실값을 가지는 모델(best model)을 깊은 복사 후 저장, 나중에 모델 불러오기 가능\n",
    "        # state_dict()는 key-value로 가중치 모델을 반환함.\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "    else:   # 가장 낮은 손실값을 가지는 에포크에서 시작해서 early_stop(100회)만큼 실행했는데 \n",
    "            # 손실값이 오히려 올라가면 학습이 무의미하다.\n",
    "        if early_stop > 0 and lowest_epoch + early_stop < i + 1:\n",
    "            print(\"There is no improvement during last %d epochs.\" % early_stop)\n",
    "            break\n",
    "\n",
    "print(\"The best validation loss from epoch %d: %.4e\" % (lowest_epoch + 1, lowest_loss))\n",
    "\n",
    "# Load best epoch's model.\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "plot_from = 10\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.grid(True)\n",
    "plt.title(\"Train / Valid Loss History\")\n",
    "plt.plot(\n",
    "    range(plot_from, len(train_history)), train_history[plot_from:],\n",
    "    range(plot_from, len(valid_history)), valid_history[plot_from:],\n",
    ")\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# 평가(테스트) 시작\n",
    "test_loss = 0\n",
    "y_hat = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_ = x[2].split(batch_size, dim=0)\n",
    "    y_ = y[2].split(batch_size, dim=0)\n",
    "\n",
    "    for x_i, y_i in zip(x_, y_):\n",
    "        y_hat_i = model(x_i)\n",
    "        loss = F.mse_loss(y_hat_i, y_i)\n",
    "\n",
    "        test_loss += loss # Gradient is already detached.\n",
    "\n",
    "        y_hat += [y_hat_i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test_loss / len(x_)\n",
    "y_hat = torch.cat(y_hat, dim=0)\n",
    "\n",
    "# 훈련, 검증 손실값들을 한데 묶어 정렬\n",
    "sorted_history = sorted(zip(train_history, valid_history), key=lambda x: x[1])\n",
    "\n",
    "print(\"Train loss: %.4e\" % sorted_history[0][0])\n",
    "print(\"Valid loss: %.4e\" % sorted_history[0][1])\n",
    "print(\"Test loss: %.4e\" % test_loss)\n",
    "\n",
    "df = pd.DataFrame(torch.cat([y[2], y_hat], dim=1).detach().numpy(),\n",
    "                  columns=[\"y\", \"y_hat\"])\n",
    "\n",
    "sns.pairplot(df, height=5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4be86c8dc50e04ba973d8ce46680464486089f3e39fa7122143c61b5c28756ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
